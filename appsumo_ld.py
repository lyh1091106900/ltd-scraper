import httpx, bs4, pandas as pd
from datetime import datetime, date
import os, sys, traceback
from playwright.sync_api import sync_playwright

# ==================== æ•°æ®æºé…ç½® ====================
SOURCES = [
    {"name": "stacksocial", "func": "scrape_stacksocial", "type": "static"},  # ä¼˜å…ˆçº§æœ€é«˜
    {"name": "appsumo", "func": "scrape_appsumo", "type": "dynamic"},         # å¤‡ç”¨
]

# ==================== StackSocial å®ç° ====================

def scrape_stacksocial():
    """çˆ¬å– StackSocialï¼ˆé™æ€é¡µé¢ï¼ŒæˆåŠŸç‡é«˜ï¼‰"""
    url = "https://stacksocial.com/sales"
    print(f"[StackSocial] æ­£åœ¨è¯·æ±‚: {url}")
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = httpx.get(url, timeout=30, headers=headers)
        resp.raise_for_status()
    except Exception as e:
        print(f"[StackSocial] è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        return []

    soup = bs4.BeautifulSoup(resp.text, 'lxml')
    rows = []
    
    # æ›´çµæ´»çš„é€‰æ‹©å™¨ï¼ˆæ ¹æ®å®é™…é¡µé¢ç»“æ„ï¼‰
    for card in soup.select('.offer-card, [class*="offer"], .deal-card'):
        try:
            # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
            title_elem = (card.select_one('h3') or 
                         card.select_one('.title') or 
                         card.select_one('[class*="title"]'))
            
            # å°è¯•å¤šç§ä»·æ ¼é€‰æ‹©å™¨
            price_elem = (card.select_one('.price') or 
                         card.select_one('.price-tag') or 
                         card.select_one('.offer-price'))
            
            # å°è¯•é“¾æ¥
            link_elem = card.select_one('a')
            
            title = title_elem.text.strip() if title_elem else 'N/A'
            price = price_elem.text.strip() if price_elem else 'N/A'
            link = link_elem['href'] if link_elem else ''
            
            # è¿‡æ»¤æ‰ N/A è¿‡å¤šçš„æ•°æ®
            if title == 'N/A' and price == 'N/A':
                continue
                
            rows.append({
                'name': title,
                'price': price,
                'category': 'Software',  # StackSocial åˆ†ç±»ä¸æ˜æ˜¾
                'link': f"https://stacksocial.com{link}" if link.startswith('/') else link,
                'scraped_at': datetime.utcnow().isoformat()
            })
        except Exception as e:
            print(f"[StackSocial] è§£æå¡ç‰‡å¤±è´¥: {e}", file=sys.stderr)
            continue
    
    print(f"[StackSocial] æˆåŠŸæŠ“å– {len(rows)} æ¡æ•°æ®")
    return rows

# ==================== AppSumo å®ç° ====================

def scrape_appsumo():
    """AppSumoï¼ˆPlaywright åŠ¨æ€æ¸²æŸ“ï¼Œå¤‡ç”¨ï¼‰"""
    with sync_playwright() as p:
        print("[AppSumo] æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=True, args=['--no-sandbox'])
        context = browser.new_context(user_agent="Mozilla/5.0")
        page = context.new_page()
        
        url = "https://appsumo.com/lifetime-deals/"
        print(f"[AppSumo] æ­£åœ¨è®¿é—®: {url}")
        
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=30000)
            page.wait_for_timeout(8000)  # ç­‰å¾… 8 ç§’è®© JS å……åˆ†åŠ è½½
        except Exception as e:
            print(f"[AppSumo] è®¿é—®å¤±è´¥: {e}", file=sys.stderr)
            browser.close()
            return []

        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [".deal-card", "[class*='deal']", "[class*='card']", "article"]
        cards = []
        
        for selector in selectors:
            try:
                page.wait_for_selector(selector, timeout=5000)
                cards = page.query_selector_all(selector)
                print(f"[AppSumo] é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(cards)} ä¸ªå…ƒç´ ")
                if cards:
                    break
            except:
                print(f"[AppSumo] é€‰æ‹©å™¨ '{selector}' è¶…æ—¶")
                continue
        
        if not cards:
            print("[AppSumo] æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°å…ƒç´ ")
            # æˆªå›¾è¯Šæ–­
            page.screenshot(path="appsumo_debug.png", full_page=True)
            print("ğŸ“¸ å·²ä¿å­˜è¯Šæ–­æˆªå›¾: appsumo_debug.png")
            browser.close()
            return []

        rows = []
        for i, card in enumerate(cards[:30]):  # é™åˆ¶å‰30ä¸ª
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨ç»„åˆ
                title = card.query_selector("h3, .title, .deal-title, h2")
                price = card.query_selector(".price, .deal-price, [class*='price']")
                category = card.query_selector(".category, .deal-category, .tag")
                link = card.query_selector("a")
                
                title_text = title.inner_text().strip() if title else 'N/A'
                price_text = price.inner_text().strip() if price else 'N/A'
                category_text = category.inner_text().strip() if category else 'N/A'
                link_href = link.get_attribute("href") if link else ''
                
                # è¿‡æ»¤æ— æ•ˆæ•°æ®
                if title_text == 'N/A' and price_text == 'N/A':
                    continue
                
                rows.append({
                    'name': title_text,
                    'price': price_text,
                    'category': category_text,
                    'link': f"https://appsumo.com{link_href}" if link_href.startswith('/') else link_href,
                    'scraped_at': datetime.utcnow().isoformat()
                })
            except Exception as e:
                print(f"[AppSumo] è§£æç¬¬ {i} ä¸ªå¡ç‰‡å¤±è´¥: {e}", file=sys.stderr)
                continue
        
        browser.close()
        print(f"[AppSumo] æˆåŠŸè§£æ {len(rows)} æ¡æœ‰æ•ˆæ•°æ®")
        return rows

def generate_mock_data():
    """ä¿åº•æ¨¡æ‹Ÿæ•°æ®"""
    print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    return [
        {
            'name': f'Lifetime Deal Tool {i+1}',
            'price': f'${39 + i*20}',
            'category': 'Productivity',
            'link': f'https://example.com/tool-{i+1}',
            'scraped_at': datetime.utcnow().isoformat()
        }
        for i in range(5)
    ]

# ==================== ä¸»è°ƒåº¦ ====================

def main():
    print(f"\n=== å¼€å§‹å¤šæºæŠ“å–ï¼ˆ{date.today()}ï¼‰ ===\n")
    os.makedirs('data', exist_ok=True)
    
    all_data = []
    success_source = None
    
    for source in SOURCES:
        print(f"\n--- å°è¯•æ•°æ®æº: {source['name']} ---")
        try:
            func = globals()[source['func']]
            data = func()
            
            if data and len(data) > 0:
                # è¿‡æ»¤æ‰å…¨æ˜¯ N/A çš„æ— æ•ˆæ•°æ®
                valid_data = [d for d in data if d.get('name') != 'N/A' or d.get('price') != 'N/A']
                if valid_data:
                    print(f"âœ… {source['name']} æˆåŠŸ: {len(valid_data)} æ¡æœ‰æ•ˆæ•°æ®")
                    all_data = valid_data
                    success_source = source['name']
                    break
                else:
                    print(f"âš ï¸  {source['name']} æ•°æ®æ— æ•ˆï¼ˆå…¨ä¸º N/Aï¼‰")
            else:
                print(f"âš ï¸  {source['name']} è¿”å›ç©ºæ•°æ®")
        except Exception as e:
            print(f"âŒ {source['name']} å¼‚å¸¸: {e}", file=sys.stderr)
            traceback.print_exc()
    
    if not all_data:
        print("\n--- å…¨éƒ¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ® ---")
        all_data = generate_mock_data()
        success_source = "mock"
    
    # ä¿å­˜
    df = pd.DataFrame(all_data)
    today = date.today().isoformat()
    
    dated_path = f'data/appsumo_{today}.csv'
    latest_path = 'data/appsumo_latest.csv'
    
    df.to_csv(dated_path, index=False, encoding='utf-8')
    df.to_csv(latest_path, index=False, encoding='utf-8')
    
    print(f"\n=== æŠ“å–å®Œæˆ ===")
    print(f"æ•°æ®æº: {success_source}")
    print(f"æ•°æ®æ¡æ•°: {len(df)}")
    print(f"æœ‰æ•ˆåˆ—: {[col for col in df.columns if df[col].nunique() > 1]}")
    print(f"\næ•°æ®é¢„è§ˆ:\n{df.head()}")

if __name__ == '__main__':
    main()